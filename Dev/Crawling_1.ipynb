{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+++\n",
    "title = \"Crawling with BeautifulSoup\"\n",
    "id = \"Dev\"\n",
    "+++\n",
    "\n",
    "# Crawling with requests and BeautifulSoup \n",
    "\n",
    "> 크롤링 할 때 마다, 자잘한 메서드 이름 까먹어서 이것 저것 이전 작업물들을 열어보게 된다. 한 곳에 모아놓기 1편 BeautifulSoup \n",
    "\n",
    "> 개인적으로는 beautifulsoup 으로 할 수 있는 작업 이면, 이것으로 하는 것을 더 좋아한다.(이유는 더 가볍고 **빠르니까!!**)\n",
    "\n",
    "> 그러나, 정보를 입력 하는 등의 웹 상에서의 행동이 필요하다면 Selenium을 쓴다. 대부분은 bs로 가능하던데 그 때 그 때 판단해서 사용한다. \n",
    "\n",
    "> * 이용 추천 - API(있다면, 이걸 쓰는게 가장 굿) -> bs -> Selenium(정말 안되면 사용하라, 브라우져를 직접 씌우니까 무겁다.)\n",
    "\n",
    "****Flow****\n",
    "\n",
    "1. Requests로 웹 정보를 받아온다.(html, xml, json $\\cdots$)\n",
    "\n",
    "2. Beautifulsoup으로 html을 파싱한다. 또는 json 파싱을 한다. (xml은 다른 라이브러리를 쓰는게 좋다.[elementtree](https://www.datacamp.com/community/tutorials/python-xml-elementtree))\n",
    "\n",
    "3. 원하는 정보를 얻는다 ! \n",
    "\n",
    "### Requests\n",
    "- http://docs.python-requests.org/en/master/\n",
    "\n",
    "--------------------------------\n",
    "** < practical example > ** \n",
    "##### 네이버 주식 데이터 가져오기\n",
    "- api 사용 : json 파싱을 한다. (*파싱 : Data 형태를 바꿔준다.*)\n",
    "\n",
    "- 네이버 주식 페이지에서 주식 데이터를 가져와 데이터 프레임으로 만들기\n",
    "- http://m.stock.naver.com\n",
    "\n",
    "\n",
    "networks -> XHR -> header -> request URL -> http://m.stock.naver.com/api/mystock/getMyGroupNameList.nhn?1517905359444\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>종목</th>\n",
       "      <th>시세</th>\n",
       "      <th>전일비</th>\n",
       "      <th>등락율</th>\n",
       "      <th>시가총액</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>대우조선해양</td>\n",
       "      <td>27300</td>\n",
       "      <td>300</td>\n",
       "      <td>1.11</td>\n",
       "      <td>29267</td>\n",
       "      <td>409962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>현대엘리베이</td>\n",
       "      <td>107000</td>\n",
       "      <td>-500</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>28862</td>\n",
       "      <td>1246867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>유한양행</td>\n",
       "      <td>235500</td>\n",
       "      <td>-3000</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>28753</td>\n",
       "      <td>32390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>한화</td>\n",
       "      <td>38300</td>\n",
       "      <td>-700</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>28709</td>\n",
       "      <td>356358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>GS리테일</td>\n",
       "      <td>37200</td>\n",
       "      <td>-50</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>28644</td>\n",
       "      <td>158686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        종목      시세    전일비   등락율   시가총액      거래량\n",
       "95  대우조선해양   27300    300  1.11  29267   409962\n",
       "96  현대엘리베이  107000   -500 -0.47  28862  1246867\n",
       "97    유한양행  235500  -3000 -1.26  28753    32390\n",
       "98      한화   38300   -700 -1.79  28709   356358\n",
       "99   GS리테일   37200    -50 -0.13  28644   158686"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_url(pageSize=10, page=1):\n",
    "    return \"http://m.stock.naver.com/api/json/sise/siseListJson.nhn?menu=market_sum&sosok=0&pageSize=\" + str(pageSize) + \"&page=\" + str(page)\n",
    "\n",
    "def get_data(url):\n",
    "    response = requests.get(url)\n",
    "    json_info = response.json()\n",
    "    companys = json_info[\"result\"][\"itemList\"]\n",
    "    df = pd.DataFrame(columns=[\"종목\", \"시세\", \"전일비\", \"등락율\", \"시가총액\", \"거래량\"])\n",
    "    for company in companys:\n",
    "        df.loc[len(df)] = {\n",
    "            \"종목\":company[\"nm\"],\n",
    "            \"시세\":company[\"nv\"],\n",
    "            \"전일비\":company[\"cv\"],\n",
    "            \"등락율\":company[\"cr\"],\n",
    "            \"시가총액\":company[\"mks\"],\n",
    "            \"거래량\":company[\"aq\"],\n",
    "        }\n",
    "    return df\n",
    "\n",
    "\n",
    "url = make_url(100,1)\n",
    "df = get_data(url)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "client_key = 'CsODwdUTyG9vOI1uIeIf'\n",
    "client_secret = 'YmIx0GW8JG'\n",
    "# 별도 quote_plus() 메서드등 처리할 필요 없음. requests 객체가 알아서 해줌\n",
    "naver_url = 'https://openapi.naver.com/v1/search/news.json?query=스마트폰'\n",
    "\n",
    "header_params = {\"X-Naver-Client-Id\":client_key, \"X-Naver-Client-Secret\":client_secret}\n",
    "# headers= header_params 는 header 변경시에만 필요하고, 그렇지 않으면, requests.get(원하는 URL) 만 해도 됨\n",
    "response = requests.get(naver_url, headers = header_params)\n",
    "# 별도 json.loads() 라이브러리 메서드 사용하지 않아도, reqeusts 라이브러리에 있는 json() 메서드로 간단히 처리 가능함\n",
    "# print(response.json())\n",
    "# print(response.text)\n",
    "\n",
    "# HTTP 응답 코드는 status_code 에 저장됨\n",
    "if(response.status_code == 200):\n",
    "    data = response.json()\n",
    "    print(data['items'][0]['title'])\n",
    "    print(data['items'][0]['description'])\n",
    "else:\n",
    "    print(\"Error Code:\" + response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "HTML을 가져와서, parsing 을 해주는 역할은 BeautifulSoup이 수행, 보통 Css selector, Xpath를 이용해서 원하는 정보에 접근 한다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">[1]크롤링이란?</h1>\n",
      "[1]크롤링이란?\n",
      "[1]크롤링이란?\n",
      "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "<h1 id=\"title\">[1]크롤링이란?</h1>\n",
      "[1]크롤링이란?\n",
      "[1]크롤링이란?\n",
      "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "<p align=\"center\" id=\"body\">파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n",
      "[<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>, <p align=\"center\" id=\"body\">파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>]\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n",
      "['오대석']\n",
      "['[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI', '오대석']\n",
      "[]\n",
      "[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI | Daum 뉴스\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = \"<html> \\\n",
    "            <body> \\\n",
    "                <h1 id='title'>[1]크롤링이란?</h1> \\\n",
    "                <p class='cssstyle'>웹페이지에서 필요한 데이터를 추출하는 것</p> \\\n",
    "                <p id='body' align='center'>파이썬을 중심으로 다양한 웹크롤링 기술 발달</p> \\\n",
    "            </body> \\\n",
    "        </html>\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 태그로 검색 방법\n",
    "title_data = soup.find('h1')\n",
    "\n",
    "print(title_data)\n",
    "print(title_data.string)\n",
    "print(title_data.get_text())\n",
    "\n",
    "# 가장 먼저 검색되는 태그를 반환\n",
    "paragraph_data = soup.find('p')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())\n",
    "\n",
    "\n",
    "# 태그에 있는 id로 검색 (javascript 예를 상기!)\n",
    "title_data = soup.find(id='title')\n",
    "\n",
    "print(title_data)\n",
    "print(title_data.string)\n",
    "print(title_data.get_text())\n",
    "\n",
    "\n",
    "# HTML 태그와 CSS class를 활용해서 필요한 데이터를 추출하는 방법1\n",
    "paragraph_data = soup.find('p', class_='cssstyle')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())\n",
    "\n",
    "\n",
    "# HTML 태그와 CSS class를 활용해서 필요한 데이터를 추출하는 방법2\n",
    "paragraph_data = soup.find('p', 'cssstyle')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())\n",
    "\n",
    "\n",
    "# HTML 태그와 태그에 있는 속성:속성값을 활용해서 필요한 데이터를 추출하는 방법\n",
    "paragraph_data = soup.find('p', attrs = {'align': 'center'})\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())\n",
    "\n",
    "# find_all() 관련된 모든 데이터를 리스트 형태로 추출하는 함수\n",
    "paragraph_data = soup.find_all('p')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data[0].get_text())\n",
    "print(paragraph_data[1].get_text())\n",
    "\n",
    "# * **string 검색**\n",
    "#  - 태그가 아닌 문자열 자체로 검색\n",
    "#  - 문자열, 정규표현식 등등으로 검색 가능\n",
    "#    - 문자열 검색의 경우 한 태그내의 문자열과 exact matching인 것만 추출\n",
    "#    - 이것이 의도한 경우가 아니라면 정규표현식 사용\n",
    "\n",
    "res = requests.get('http://v.media.daum.net/v/20170518153405933')\n",
    "soup = BeautifulSoup(res.content, 'html5lib')\n",
    "\n",
    "print (soup.find_all(string='오대석'))\n",
    "print (soup.find_all(string=['[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI', '오대석']))\n",
    "print (soup.find_all(string='AI'))\n",
    "print (soup.find_all(string=re.compile('AI'))[0])\n",
    "# print (soup.find_all(string=re.compile('AI')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *< practical example >*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Naver realtime Keywords >\n",
      "   rank    keyword\n",
      "15   16      방탄소년단\n",
      "16   17   해운대 모래축제\n",
      "17   18  콰이어트 플레이스\n",
      "18   19        심혜진\n",
      "19   20         달력\n",
      "**************************************************\n",
      "< Daum realtime Keywords >\n",
      "  rank   keyword\n",
      "5    6   부처님 오신날\n",
      "6    7  냉장고를 부탁해\n",
      "7    8        독전\n",
      "8    9       홍문종\n",
      "9   10     성년의 날\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['김부겸', '냉장고를 부탁해', '독전']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Naver 실시간 검색어 순위\n",
    "def naver_top20():\n",
    "    df = pd.DataFrame(columns=[\"rank\",\"keyword\"])\n",
    "    response = requests.get(\"http://naver.com\")\n",
    "    dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "    keywords = dom.select(\".ah_roll .ah_l .ah_item\")\n",
    "    for keyword in keywords:\n",
    "        df.loc[len(df)] = {\n",
    "            \"rank\":keyword.select_one(\".ah_r\").text,\n",
    "            \"keyword\":keyword.select_one(\".ah_k\").text,\n",
    "        }\n",
    "    return df\n",
    "print(\"< Naver realtime Keywords >\")\n",
    "naver_df = naver_top20()\n",
    "pp.pprint(naver_df.tail(5))\n",
    "\n",
    "# 다음 실시간 검색어 순위 \n",
    "def daum_top10():\n",
    "    df = pd.DataFrame(columns=[\"rank\",\"keyword\"])\n",
    "    response = requests.get(\"http://daum.net\")\n",
    "    dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "    keywords = dom.select(\"#mArticle ol.list_hotissue.issue_row.list_mini > li\")\n",
    "    for keyword in keywords:\n",
    "        df.loc[len(df)] = {\n",
    "            \"rank\":keyword.select_one(\".ir_wa\").text.replace(\"위\",\"\"),\n",
    "            \"keyword\":keyword.select_one(\".link_issue\").text,\n",
    "        }\n",
    "    return df\n",
    "print(\"*\" * 50)\n",
    "\n",
    "print(\"< Daum realtime Keywords >\")\n",
    "daum_df = daum_top10()\n",
    "pp.pprint(daum_df.tail(5))\n",
    "\n",
    "\n",
    "# 위의 결과 중에 중복되는 검색어를 출력\n",
    "result = [keyword for keyword in daum_df[\"keyword\"] if naver_df[\"keyword\"].str.contains(keyword).any() ]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download done : 0.0 Mbyte\n"
     ]
    }
   ],
   "source": [
    "# File Download\n",
    "\n",
    "def download(title, download_link):\n",
    "    response = requests.get(download_link, stream=True)\n",
    "    download_path =\"/Users/MAC/desktop/{}\".format(title)\n",
    "    size = 0\n",
    "    with open(download_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024): \n",
    "            if chunk:\n",
    "                size += 1024\n",
    "                f.write(chunk)\n",
    "    return size\n",
    "\n",
    "\n",
    "title = \"iPhone X is Here — Apple.mp4\"\n",
    "download_link = \"http://bit.ly/2FLpRF9\"\n",
    "size = download(title, download_link)\n",
    "print(\"download done : {} Mbyte\".format(round(size/1024/1024,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한빛 미디어에 로그인\n",
    "login_url = 'http://www.hanbit.co.kr/member/login_proc.php'\n",
    "\n",
    "user = 'input Id'\n",
    "password = 'and password'\n",
    "\n",
    "# requests.session 메서드는 해당 reqeusts를 사용하는 동안 cookie를 header에 유지하도록 하여\n",
    "# 세션이 필요한 HTTP 요청에 사용됩니다.\n",
    "session = requests.session()\n",
    "\n",
    "params = dict()\n",
    "params['m_id'] = user\n",
    "params['m_passwd'] = password\n",
    "\n",
    "# javascrit(jQuery) 코드를 분석해보니, 결국 login_proc.php 를 m_id 와 m_passwd 값과 함께\n",
    "# POST로 호출하기 때문에 다음과 같이 requests.session.post() 메서드를 활용하였습니다.\n",
    "# 실제코드: <form name=\"frm\"  id=\"frm\"  action=\"#\" method=\"post\">\n",
    "res = session.post(login_url, data = params) \n",
    "\n",
    "# 응답코드가 200 즉, OK가 아닌 경우 에러를 발생시키는 메서드입니다.\n",
    "res.raise_for_status() \n",
    "\n",
    "# 'Set-Cookie'로 PHPSESSID 라는 세션 ID 값이 넘어옴을 알 수 있다.\n",
    "# print(res.headers)\n",
    "\n",
    "# cookie로 세션을 로그인 상태를 관리하는 상태를 확인해보기 위한 코드입니다.\n",
    "# print(session.cookies.get_dict()) \n",
    "\n",
    "# 여기서부터는 로그인이 된 세션이 유지됩니다. session 에 header에는 Cookie에 PHPSESSID가 들어갑니다.\n",
    "mypage_url = 'http://www.hanbit.co.kr/myhanbit/myhanbit.html'\n",
    "res = session.get(mypage_url)\n",
    "\n",
    "# 응답코드가 200 즉, OK가 아닌 경우 에러를 발생시키는 메서드입니다.\n",
    "res.raise_for_status() \n",
    "\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# Chrome 개발자 도구에서 CSS SELECTOR를 통해 간단히 가져온 CSS SELECTOR 표현식을 사용\n",
    "he_coin = soup.select_one('#container > div > div.sm_mymileage > dl.mileage_section2 > dd > span')\n",
    "\n",
    "# 다음과 같이 class를 .mileage_section2 로 그리고 그 하부 태그중에 span이 있다는 식으로 표현도 가능함\n",
    "# he_coin = soup.select_one('.mileage_section2 span')\n",
    "\n",
    "print ('mileage is', he_coin.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BeautifulSoup 으로 했던 것들\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "- 날씨 크롤링 from (http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "res = requests.get('http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=108')\n",
    "\n",
    "soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "data_by_locations = soup.find_all('location')\n",
    "\n",
    "# cloumns = ['city']\n",
    "# rows = ['tmef']\n",
    "# value = 날씨 데이터 \n",
    "#data_by_locations[0]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "#print(data_by_locations)\n",
    "time_weather = pd.Series()\n",
    "\n",
    "\n",
    "for location in data_by_locations:\n",
    "    city = location.find('city') \n",
    "    times = location.find_all('tmef')\n",
    "    values = location.find_all('wf')\n",
    "    \n",
    "    city_name = city.get_text()\n",
    "    for i in range(len(times)):\n",
    "        weather = values[i].get_text()\n",
    "        day = times[i].get_text()\n",
    "        \n",
    "        time_weather[day] = weather\n",
    "\n",
    "    df[city_name] = time_weather\n",
    "#    indexing_time = time\n",
    "#    df.set_index(time_weather.keys())\n",
    "\n",
    "df.to_csv('/Users/Mac/Desktop/weather.csv')\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<font color=\"blue\" size=\"3em\"><b>네이버 영화 댓글 크롤링 프로젝트</b></font>\n",
    "http://movie.naver.com/\n",
    "\n",
    "<font size=\"3em\"><b>요구사항:</b></font><br>\n",
    "http://movie.naver.com/movie/running/current.nhn 페이지(현재 상영영화)에서 <br>\n",
    "<br>\n",
    "(1) 예매순 1위 ~ 5위에 해당하는 영화 각각의<br>\n",
    "(2) 개봉후 평점글(140자 평)을 <br>\n",
    "(3) 호감순 100개 읽어서 출력하기<br>\n",
    "<br>\n",
    "<font size=\"3em\"><b>출력 포멧:</b></font><br>\n",
    "[영화 이름] (호감순 140자 평)<br>\n",
    "1]호감순 140자 평1<br>\n",
    "2]호감순 140자 평1<br>\n",
    "<b>.</b><br>\n",
    "<b>.</b><br>\n",
    "<b>.</b><br>\n",
    "100]호감순 140자 평1<br>\n",
    "<br>\n",
    "<font size=\"3em\"><b>고려 사항:</b></font><br>\n",
    "140자 평이 100개 이하일 경우에는 140자 평 갯수만큼 출력할 것<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://movie.naver.com/movie/running/current.nhn')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "movie_list = {}\n",
    "movies = soup.select(\"#content > div.article > div > div.lst_wrap > ul > li\")\n",
    "\n",
    "for n in range(5):\n",
    "    movie = movies[n].find_all(\"a\")\n",
    "    movie_list[movie[1].string] = \"http://movie.naver.com\"+movie[0].get('href').replace(\"basic\", \"pointWriteFormList\")+\"&type=after&page=\"\n",
    "        \n",
    "for movie_name, movie_href in movie_list.items():\n",
    "    print('[{}] (호감순 140자 평)'.format(movie_name))\n",
    "    comments = []\n",
    "    i=0\n",
    "    loop = True\n",
    "    while loop and i < 10:\n",
    "        i+=1\n",
    "        try:\n",
    "            res = requests.get(movie_href+str(i))\n",
    "            soup = BeautifulSoup(res.content, 'html.parser')\n",
    "            comment_list = soup.select(\"body > div > div > div.score_result > ul\")\n",
    "            for comment in comment_list[0].find_all(\"p\"):\n",
    "                comments.append(comment.text.replace('BEST','').replace('관람객',''))\n",
    "        except:\n",
    "            loop = False\n",
    "        \n",
    "    for i, comment in enumerate(comments):\n",
    "        print('{}] {}'.format(i+1, comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네이버 경제 기사 크롤링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests \n",
    "import re \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Get the news for an year\n",
    "for i in range(365):\n",
    "    today = datetime.date.today()\n",
    "    today = today + datetime.timedelta(-i)\n",
    "    today = (\"\").join(str(today).split('-'))\n",
    "    print(today)\n",
    "    naver_news_today= \"http://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&sectionId=101&date={}\".format(today)\n",
    "    print(naver_news_today)\n",
    "    res = requests.get(naver_news_today)\n",
    "    soup = bs(res.content, 'html.parser')\n",
    "    try:\n",
    "    # get rinks\n",
    "        rinks = []\n",
    "        # top3\n",
    "        top3 = soup.find('div', class_=\"ranking_top3\")\n",
    "        top3 = top3.select(\"a[href]\")\n",
    "        for top in top3:\n",
    "            rinks.append(re.search('/main/\\w+/\\w+[.a-zA-Z?=&0-9;_]+', top['href']).group())\n",
    "        rinks = rinks[::2]\n",
    "        # 4 to 30th ranked news\n",
    "        for rank in range(4,31):\n",
    "            all_= soup.find('li', class_=\"gnum{}\".format(rank))\n",
    "            all_ = all_.select(\"a[href]\")\n",
    "            for al in all_:\n",
    "                rinks.append(re.search('/main/\\w+/\\w+[.a-zA-Z?=&0-9_]+', al['href']).group())\n",
    "        \n",
    "        # gathering the day's news\n",
    "        articles = '{}'.format(today)\n",
    "        for rink in rinks:\n",
    "            res = requests.get(base_url+rink)\n",
    "            soup = bs(res.content, 'html.parser')\n",
    "            article = re.sub(r'[\\n]', '', soup.find(\"div\", id=\"articleBodyContents\").text)\n",
    "            article = re.sub(r'[\\w0-9a-zA-Z_.-]@[\\w0-9a-zA-Z_.-]', '', article)\n",
    "            article = re.sub(r'\\w+\\.\\w+.\\w+', '', article)\n",
    "            article = re.sub(r'무단전재 및 재배포 금지', '', article)\n",
    "            article = re.sub(r'▶.*', '', article )\n",
    "            articles += article \n",
    "        \n",
    "        # make txt files\n",
    "        with open(\"article{}.txt\".format(i), \"w\") as f:\n",
    "            f.write(articles)\n",
    "        \n",
    "    except : \n",
    "        print(\"{} is missed\".format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced requests\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "HTTP Request를 보낼때 2가지 중요한 구성요소가 있다. 바로 Request의 header와 body.\n",
    "\n",
    "body는 실제 컨텐츠들이 들어가는 곳이고 header는 body를 위한 다양한 정보를 가지고 있는 곳이다 (encoding type, cookies, request method. 등등) 이러한 header의 정보중 'Content-Length'라고 body의 사이즈를 정하는게 있다. \n",
    "\n",
    " 우리가 웹사이트를 만들때 이것들을 일일히 명시하지 않았더라도 framework가 우릴 위해 알아서 지정해 줬을 것이다. 이전에 클라이언트에게 response를 보낸적이 있다면, framework가 response의 사이즈를 측정해 이 header에 적용한다.\n",
    " \n",
    "#### Chunk Response란 ? \n",
    " Chunk Response, \"덩어리 응답\"은 전체 페이지를 가공하지 않고. 즉, 서버측에서 html을 전부 생성한 후에 클라이언트에게 보내는 것이 아니라 html을 덩어리(chunk) 단위로 쪼개서 보낼 수 있다. 브라우저에게 전체 컨텐츠 크기가 얼마나 큰지 알려주지 않아도됨. \n",
    "\n",
    " 따라서, 동적인 크기의 컨텐츠에 적합함. 스트리밍에도 좋다.  이러한 방식의 응답은 Chunked transfer encoding을 사용해야함. \n",
    " \n",
    "#### Chunked transfer encoding은 ?\n",
    "\n",
    " HTTP 버전 1.1 데이터 전송 메커니즘 중 하나. 덩어리(Chunk)의 나열로 데이터를 전송한다는 것.\n",
    "HTTP 헤더의 Content-Length 대신 Transfer-Encoding을 사용. (이전버전에서는 다른 처리가 필요하다고함)\n",
    "\n",
    "Content-Length 속성을 사용하지 않기 때문에, Receiver에 대한 응답을 전송하기 전에 Sender는 컨텐츠의 길이를 알 필요가 없다. Sender는 그 컨텐츠의 전체 크기를 알기 전에 동적으로 생성한 컨텐츠를 전송할 수 있다.\n",
    "\n",
    " 이건 이해를 위해 예를 들어보자면, (극적인 예)기존에는 서버에서 동영상을 인코딩을 다 하고 난 후, 동영상의 사이즈를 파악한 후에 전송할 수 있었다면, Chunked transfer encoding은 전체 크기를 알 필요가 없고 조각조각 보내기 때문에 가공을 하는 중간중간에도 계속 데이터를 전송할 수 있다는 의미.  \n",
    "\n",
    "\n",
    "```\n",
    "형태\n",
    "\n",
    "[크기 16진수]\\r\\n\n",
    "\n",
    "[data]\\r\\n\n",
    "\n",
    "\n",
    "Encode 예제\n",
    "\n",
    "4\\r\\n\n",
    "\n",
    "Wiki\\r\\n\n",
    "\n",
    "5\\r\\n\n",
    "\n",
    "pedia\\r\\n\n",
    "\n",
    "e\\r\\n\n",
    "\n",
    " in\\r\\n\\r\\nchunks.\\r\\n\n",
    " \n",
    "0\\r\\n\n",
    "\n",
    "\\r\\n\n",
    "\n",
    " Decode\n",
    "\n",
    "Wikipedia in\n",
    "\n",
    "chunks.\n",
    "```\n",
    "\n",
    "www.amazon.com에서도 이 기술을 활용하고 있다.\n",
    "\n",
    "\n",
    "아마존에서 적용한 사례 Transfer-Encdoing: chunked와 content의 크기를 d7c(16진수)로 표시한것을 확인할 수 있다.\n",
    "![](http://cfile28.uf.tistory.com/image/2560394753FF501A1D6319)\n",
    "\n",
    "\n",
    "#### 어느상황에서 사용하지?\n",
    "\n",
    "어떤 사이트의 페이지들은 긴 처리시간이 필요하다. 서버가 response를 만드는 동안에 사용자는 흰색 화면만 볼것이고 불쌍한 브라우저는 이 시간동안 아무것도 하지 못한다. \n",
    "\n",
    "\n",
    "\n",
    "Chunked Response를 쓰면! 컨텐츠의 특정 부분을 생성할 수 있고 클라이언트에게 미리 던져 줄 수 있다. HTML <head>태그안에 script와 stylesheet들이 있다면 사용자의 브라우저에 <html>태그 안의 정보만 미리 던져 줄 수 있다. 그 동안 서버는 DB에서 데이터를 가져오든 뭘하든 투닥투닥 나머지 부분을 생성할 수 있고, 브라우저는 head의 자원들을 해석하고 있을 수 있다.  병렬적으로 둘다 일을 할 수 있는것이다!! (우오오오)\n",
    "\n",
    "\n",
    "\n",
    "심지어 사용자자가 컨텐츠의 일부만 본다거나 할때처럼 이러한 처리가 적합하지 않다고 해도 사용자는 더 나은 성능인것 같은 '느낌'을 받을 수 있다.  'perceived performance' 효과라고 한다고 한다고 하는데 ...이것도 봐야지..\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Server': 'nginx', 'Date': 'Thu, 24 May 2018 00:44:56 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding', 'P3P': 'CP=\"CAO DSP LAW CUR ADM DEV TAI PSA IVAo CONo HISo OTP OUR DEL LEG SAMo UNI COM PUR NAV INT STA\"', 'Set-Cookie': 'PHPSESSID=164vp2mlt4g9ekqtdvnim76k45; path=/; domain=.musinsa.com, bizest=0evUxkQMnoxW2lGEF1r5m6x9FPG07dshrEyurIh5rD9GIZ9haUYtcWC9mVvoI1w79RiifN77VZX9CbXASy6NvZ58X58FWV9YkOTXjiaaFWzEPTaXProsll7i8jp2Ihgy5G0MTpWfV6diaIHUCSeEp5YdGe%2F0keC1g%2B%2B%2B0Cx%2FmITP1sVo2egeOX8x9ZHwVDKBd7nWtPdd72jbH4Ymo5OcZlw83ciseppQqrvRl5s4KnwKCsBKvSiq1MyVxdr%2Blv9Buu%2B1rSgFo%2Bkz%2FsT7Q2b3Ie5QT4b60yV9vaxBcWwvR8xD7rwCjbwDQIK%2FU9MmEIvzkj3Mwy268oVgwBq9vkgqFn1EaOaam0fUDsWuOO5X1iu8a5nGiewNEkDxL%2B6cyV6bP%2Fcpoy3D1%2Bu2AcNRC02NTSmGYHSITYhVZdITiK6Y6Zgmz8BKQLIqcTQ0LPXtTpX727R%2F2aGL93tEr%2BOx1qwHvKv2Hi2ad1%2FG9SOf2BUTP%2Bpg82vJ7uFdjFMq%2F6Z5C75jN778SBfex1winuODRnZuFBBWkrgKeDUR6idRys4ct7w%3D; path=/; domain=.musinsa.com, tr[vid]=5b060b08beb46; expires=Fri, 24-May-2019 00:44:56 GMT; Max-Age=31536000; path=/, tr[vd]=1527122696; expires=Fri, 24-May-2019 00:44:56 GMT; Max-Age=31536000; path=/, tr[vc]=1; expires=Fri, 25-May-2018 00:44:56 GMT; Max-Age=86400; path=/, tr[vt]=1527122696; expires=Thu, 24-May-2018 01:04:56 GMT; Max-Age=1200; path=/, tr[pv]=1; expires=Thu, 24-May-2018 01:04:56 GMT; Max-Age=1200; path=/, tr[fr]=deleted; expires=Thu, 01-Jan-1970 00:00:01 GMT; Max-Age=0; path=/', 'Expires': 'Thu, 19 Nov 1981 08:52:00 GMT', 'Cache-Control': 'no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Pragma': 'no-cache', 'Content-Encoding': 'gzip', 'X-XSS-Protection': '1; mode=block'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#  상의 - 가격  과 별점 , 이미지 까지 한번에 크롤링 \n",
    "page = 1 \n",
    "res = requests.get(\"http://store.musinsa.com/app/items/lists/001001/?category=&d_cat_cd=001001&u_cat_cd=&brand=&sort=pop&display_cnt=120&page={}&page_kind=category&list_kind=small&free_dlv=&ex_soldout=&sale_goods=&exclusive_yn=&price=&color=&a_cat_cd=&sex=&size=&tag=&popup=&brand_favorite_yn=&goods_favorite_yn=&=&price1=&price2=&brand_favorite=&goods_favorite=&chk_exclusive=&chk_sale=&chk_soldout=\".format(page))\n",
    "res.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Response' object has no attribute 'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c7c0b7f1fe51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Response' object has no attribute 'body'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
