{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+++\n",
    "title = \"Crawling with BeautifulSoup\"\n",
    "id = \"Dev\"\n",
    "+++\n",
    "\n",
    "# Crawling with requests and BeautifulSoup \n",
    "\n",
    "> 크롤링 할 때 마다, 자잘한 메서드 이름 까먹어서 이것 저것 이전 작업물들을 열어보게 된다. 한 곳에 모아놓기 1편 BeautifulSoup \n",
    "\n",
    "> 개인적으로는 beautifulsoup 으로 할 수 있는 작업 이면, 이것으로 하는 것을 더 좋아한다.(이유는 더 가볍고 **빠르니까!!**)\n",
    "\n",
    "> 그러나, 정보를 입력 하는 등의 웹 상에서의 행동이 필요하다면 Selenium을 쓴다. 대부분은 bs로 가능하던데 그 때 그 때 판단해서 사용한다. \n",
    "\n",
    "> * 이용 추천 - API(있다면, 이걸 쓰는게 가장 굿) -> bs -> Selenium(정말 안되면 사용하라, 브라우져를 직접 씌우니까 무겁다.)\n",
    "\n",
    "****Flow****\n",
    "\n",
    "1. Requests로 웹 정보를 받아온다.(html, xml, json $\\cdots$)\n",
    "\n",
    "2. Beautifulsoup으로 html을 파싱한다. 또는 json 파싱을 한다. (xml은 다른 라이브러리를 쓰는게 좋다.[elementtree](https://www.datacamp.com/community/tutorials/python-xml-elementtree))\n",
    "\n",
    "3. 원하는 정보를 얻는다 ! \n",
    "\n",
    "### Requests\n",
    "- http://docs.python-requests.org/en/master/\n",
    "\n",
    "--------------------------------\n",
    "** < practical example > ** \n",
    "##### 네이버 주식 데이터 가져오기\n",
    "- api 사용 : json 파싱을 한다. (*파싱 : Data 형태를 바꿔준다.*)\n",
    "\n",
    "- 네이버 주식 페이지에서 주식 데이터를 가져와 데이터 프레임으로 만들기\n",
    "- http://m.stock.naver.com\n",
    "\n",
    "\n",
    "networks -> XHR -> header -> request URL -> http://m.stock.naver.com/api/mystock/getMyGroupNameList.nhn?1517905359444\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "** < practical example > ** \n",
    "##### 네이버 주식 데이터 가져오기\n",
    "- api 사용 : json 파싱을 한다. (*파싱 : Data 형태를 바꿔준다.*)\n",
    "\n",
    "- 네이버 주식 페이지에서 주식 데이터를 가져와 데이터 프레임으로 만들기\n",
    "- http://m.stock.naver.com\n",
    "\n",
    "\n",
    "\n",
    "networks -> XHR -> header -> request URL -> http://m.stock.naver.com/api/mystock/getMyGroupNameList.nhn?1517905359444\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>종목</th>\n",
       "      <th>시세</th>\n",
       "      <th>전일비</th>\n",
       "      <th>등락율</th>\n",
       "      <th>시가총액</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>대우조선해양</td>\n",
       "      <td>27300</td>\n",
       "      <td>300</td>\n",
       "      <td>1.11</td>\n",
       "      <td>29267</td>\n",
       "      <td>409962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>현대엘리베이</td>\n",
       "      <td>107000</td>\n",
       "      <td>-500</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>28862</td>\n",
       "      <td>1246867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>유한양행</td>\n",
       "      <td>235500</td>\n",
       "      <td>-3000</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>28753</td>\n",
       "      <td>32390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>한화</td>\n",
       "      <td>38300</td>\n",
       "      <td>-700</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>28709</td>\n",
       "      <td>356358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>GS리테일</td>\n",
       "      <td>37200</td>\n",
       "      <td>-50</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>28644</td>\n",
       "      <td>158686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        종목      시세    전일비   등락율   시가총액      거래량\n",
       "95  대우조선해양   27300    300  1.11  29267   409962\n",
       "96  현대엘리베이  107000   -500 -0.47  28862  1246867\n",
       "97    유한양행  235500  -3000 -1.26  28753    32390\n",
       "98      한화   38300   -700 -1.79  28709   356358\n",
       "99   GS리테일   37200    -50 -0.13  28644   158686"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_url(pageSize=10, page=1):\n",
    "    return \"http://m.stock.naver.com/api/json/sise/siseListJson.nhn?menu=market_sum&sosok=0&pageSize=\" + str(pageSize) + \"&page=\" + str(page)\n",
    "\n",
    "def get_data(url):\n",
    "    response = requests.get(url)\n",
    "    json_info = response.json()\n",
    "    companys = json_info[\"result\"][\"itemList\"]\n",
    "    df = pd.DataFrame(columns=[\"종목\", \"시세\", \"전일비\", \"등락율\", \"시가총액\", \"거래량\"])\n",
    "    for company in companys:\n",
    "        df.loc[len(df)] = {\n",
    "            \"종목\":company[\"nm\"],\n",
    "            \"시세\":company[\"nv\"],\n",
    "            \"전일비\":company[\"cv\"],\n",
    "            \"등락율\":company[\"cr\"],\n",
    "            \"시가총액\":company[\"mks\"],\n",
    "            \"거래량\":company[\"aq\"],\n",
    "        }\n",
    "    return df\n",
    "\n",
    "\n",
    "url = make_url(100,1)\n",
    "df = get_data(url)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "client_key = 'CsODwdUTyG9vOI1uIeIf'\n",
    "client_secret = 'YmIx0GW8JG'\n",
    "# 별도 quote_plus() 메서드등 처리할 필요 없음. requests 객체가 알아서 해줌\n",
    "naver_url = 'https://openapi.naver.com/v1/search/news.json?query=스마트폰'\n",
    "\n",
    "header_params = {\"X-Naver-Client-Id\":client_key, \"X-Naver-Client-Secret\":client_secret}\n",
    "# headers= header_params 는 header 변경시에만 필요하고, 그렇지 않으면, requests.get(원하는 URL) 만 해도 됨\n",
    "response = requests.get(naver_url, headers = header_params)\n",
    "# 별도 json.loads() 라이브러리 메서드 사용하지 않아도, reqeusts 라이브러리에 있는 json() 메서드로 간단히 처리 가능함\n",
    "# print(response.json())\n",
    "# print(response.text)\n",
    "\n",
    "# HTTP 응답 코드는 status_code 에 저장됨\n",
    "if(response.status_code == 200):\n",
    "    data = response.json()\n",
    "    print(data['items'][0]['title'])\n",
    "    print(data['items'][0]['description'])\n",
    "else:\n",
    "    print(\"Error Code:\" + response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "HTML을 가져와서, parsing 을 해주는 역할은 BeautifulSoup이 수행, 보통 Css selector, Xpath를 이용해서 원하는 정보에 접근 한다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">[1]크롤링이란?</h1>\n",
      "[1]크롤링이란?\n",
      "[1]크롤링이란?\n",
      "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "<h1 id=\"title\">[1]크롤링이란?</h1>\n",
      "[1]크롤링이란?\n",
      "[1]크롤링이란?\n",
      "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "<p align=\"center\" id=\"body\">파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n",
      "[<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>, <p align=\"center\" id=\"body\">파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>]\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n",
      "['오대석']\n",
      "['[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI', '오대석']\n",
      "[]\n",
      "[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI | Daum 뉴스\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = \"<html> \\\n",
    "            <body> \\\n",
    "                <h1 id='title'>[1]크롤링이란?</h1> \\\n",
    "                <p class='cssstyle'>웹페이지에서 필요한 데이터를 추출하는 것</p> \\\n",
    "                <p id='body' align='center'>파이썬을 중심으로 다양한 웹크롤링 기술 발달</p> \\\n",
    "            </body> \\\n",
    "        </html>\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 태그로 검색 방법\n",
    "title_data = soup.find('h1')\n",
    "\n",
    "print(title_data)\n",
    "print(title_data.string)\n",
    "print(title_data.get_text())\n",
    "\n",
    "# 가장 먼저 검색되는 태그를 반환\n",
    "paragraph_data = soup.find('p')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())\n",
    "\n",
    "\n",
    "# 태그에 있는 id로 검색 (javascript 예를 상기!)\n",
    "title_data = soup.find(id='title')\n",
    "\n",
    "print(title_data)\n",
    "print(title_data.string)\n",
    "print(title_data.get_text())\n",
    "\n",
    "\n",
    "# HTML 태그와 CSS class를 활용해서 필요한 데이터를 추출하는 방법1\n",
    "paragraph_data = soup.find('p', class_='cssstyle')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())\n",
    "\n",
    "\n",
    "# HTML 태그와 CSS class를 활용해서 필요한 데이터를 추출하는 방법2\n",
    "paragraph_data = soup.find('p', 'cssstyle')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())\n",
    "\n",
    "\n",
    "# HTML 태그와 태그에 있는 속성:속성값을 활용해서 필요한 데이터를 추출하는 방법\n",
    "paragraph_data = soup.find('p', attrs = {'align': 'center'})\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())\n",
    "\n",
    "# find_all() 관련된 모든 데이터를 리스트 형태로 추출하는 함수\n",
    "paragraph_data = soup.find_all('p')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data[0].get_text())\n",
    "print(paragraph_data[1].get_text())\n",
    "\n",
    "# * **string 검색**\n",
    "#  - 태그가 아닌 문자열 자체로 검색\n",
    "#  - 문자열, 정규표현식 등등으로 검색 가능\n",
    "#    - 문자열 검색의 경우 한 태그내의 문자열과 exact matching인 것만 추출\n",
    "#    - 이것이 의도한 경우가 아니라면 정규표현식 사용\n",
    "\n",
    "res = requests.get('http://v.media.daum.net/v/20170518153405933')\n",
    "soup = BeautifulSoup(res.content, 'html5lib')\n",
    "\n",
    "print (soup.find_all(string='오대석'))\n",
    "print (soup.find_all(string=['[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI', '오대석']))\n",
    "print (soup.find_all(string='AI'))\n",
    "print (soup.find_all(string=re.compile('AI'))[0])\n",
    "# print (soup.find_all(string=re.compile('AI')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *< practical example >*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Naver realtime Keywords >\n",
      "   rank    keyword\n",
      "15   16      방탄소년단\n",
      "16   17   해운대 모래축제\n",
      "17   18  콰이어트 플레이스\n",
      "18   19        심혜진\n",
      "19   20         달력\n",
      "**************************************************\n",
      "< Daum realtime Keywords >\n",
      "  rank   keyword\n",
      "5    6   부처님 오신날\n",
      "6    7  냉장고를 부탁해\n",
      "7    8        독전\n",
      "8    9       홍문종\n",
      "9   10     성년의 날\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['김부겸', '냉장고를 부탁해', '독전']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Naver 실시간 검색어 순위\n",
    "def naver_top20():\n",
    "    df = pd.DataFrame(columns=[\"rank\",\"keyword\"])\n",
    "    response = requests.get(\"http://naver.com\")\n",
    "    dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "    keywords = dom.select(\".ah_roll .ah_l .ah_item\")\n",
    "    for keyword in keywords:\n",
    "        df.loc[len(df)] = {\n",
    "            \"rank\":keyword.select_one(\".ah_r\").text,\n",
    "            \"keyword\":keyword.select_one(\".ah_k\").text,\n",
    "        }\n",
    "    return df\n",
    "print(\"< Naver realtime Keywords >\")\n",
    "naver_df = naver_top20()\n",
    "pp.pprint(naver_df.tail(5))\n",
    "\n",
    "# 다음 실시간 검색어 순위 \n",
    "def daum_top10():\n",
    "    df = pd.DataFrame(columns=[\"rank\",\"keyword\"])\n",
    "    response = requests.get(\"http://daum.net\")\n",
    "    dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "    keywords = dom.select(\"#mArticle ol.list_hotissue.issue_row.list_mini > li\")\n",
    "    for keyword in keywords:\n",
    "        df.loc[len(df)] = {\n",
    "            \"rank\":keyword.select_one(\".ir_wa\").text.replace(\"위\",\"\"),\n",
    "            \"keyword\":keyword.select_one(\".link_issue\").text,\n",
    "        }\n",
    "    return df\n",
    "print(\"*\" * 50)\n",
    "\n",
    "print(\"< Daum realtime Keywords >\")\n",
    "daum_df = daum_top10()\n",
    "pp.pprint(daum_df.tail(5))\n",
    "\n",
    "\n",
    "# 위의 결과 중에 중복되는 검색어를 출력\n",
    "result = [keyword for keyword in daum_df[\"keyword\"] if naver_df[\"keyword\"].str.contains(keyword).any() ]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download done : 0.0 Mbyte\n"
     ]
    }
   ],
   "source": [
    "# File Download\n",
    "\n",
    "def download(title, download_link):\n",
    "    response = requests.get(download_link, stream=True)\n",
    "    download_path =\"/Users/MAC/desktop/{}\".format(title)\n",
    "    size = 0\n",
    "    with open(download_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024): \n",
    "            if chunk:\n",
    "                size += 1024\n",
    "                f.write(chunk)\n",
    "    return size\n",
    "\n",
    "\n",
    "title = \"iPhone X is Here — Apple.mp4\"\n",
    "download_link = \"http://bit.ly/2FLpRF9\"\n",
    "size = download(title, download_link)\n",
    "print(\"download done : {} Mbyte\".format(round(size/1024/1024,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한빛 미디어에 로그인\n",
    "login_url = 'http://www.hanbit.co.kr/member/login_proc.php'\n",
    "\n",
    "user = 'input Id'\n",
    "password = 'and password'\n",
    "\n",
    "# requests.session 메서드는 해당 reqeusts를 사용하는 동안 cookie를 header에 유지하도록 하여\n",
    "# 세션이 필요한 HTTP 요청에 사용됩니다.\n",
    "session = requests.session()\n",
    "\n",
    "params = dict()\n",
    "params['m_id'] = user\n",
    "params['m_passwd'] = password\n",
    "\n",
    "# javascrit(jQuery) 코드를 분석해보니, 결국 login_proc.php 를 m_id 와 m_passwd 값과 함께\n",
    "# POST로 호출하기 때문에 다음과 같이 requests.session.post() 메서드를 활용하였습니다.\n",
    "# 실제코드: <form name=\"frm\"  id=\"frm\"  action=\"#\" method=\"post\">\n",
    "res = session.post(login_url, data = params) \n",
    "\n",
    "# 응답코드가 200 즉, OK가 아닌 경우 에러를 발생시키는 메서드입니다.\n",
    "res.raise_for_status() \n",
    "\n",
    "# 'Set-Cookie'로 PHPSESSID 라는 세션 ID 값이 넘어옴을 알 수 있다.\n",
    "# print(res.headers)\n",
    "\n",
    "# cookie로 세션을 로그인 상태를 관리하는 상태를 확인해보기 위한 코드입니다.\n",
    "# print(session.cookies.get_dict()) \n",
    "\n",
    "# 여기서부터는 로그인이 된 세션이 유지됩니다. session 에 header에는 Cookie에 PHPSESSID가 들어갑니다.\n",
    "mypage_url = 'http://www.hanbit.co.kr/myhanbit/myhanbit.html'\n",
    "res = session.get(mypage_url)\n",
    "\n",
    "# 응답코드가 200 즉, OK가 아닌 경우 에러를 발생시키는 메서드입니다.\n",
    "res.raise_for_status() \n",
    "\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# Chrome 개발자 도구에서 CSS SELECTOR를 통해 간단히 가져온 CSS SELECTOR 표현식을 사용\n",
    "he_coin = soup.select_one('#container > div > div.sm_mymileage > dl.mileage_section2 > dd > span')\n",
    "\n",
    "# 다음과 같이 class를 .mileage_section2 로 그리고 그 하부 태그중에 span이 있다는 식으로 표현도 가능함\n",
    "# he_coin = soup.select_one('.mileage_section2 span')\n",
    "\n",
    "print ('mileage is', he_coin.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BeautifulSoup 으로 했던 것들\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "- 날씨 크롤링 from (http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "res = requests.get('http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=108')\n",
    "\n",
    "soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "data_by_locations = soup.find_all('location')\n",
    "\n",
    "# cloumns = ['city']\n",
    "# rows = ['tmef']\n",
    "# value = 날씨 데이터 \n",
    "#data_by_locations[0]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "#print(data_by_locations)\n",
    "time_weather = pd.Series()\n",
    "\n",
    "\n",
    "for location in data_by_locations:\n",
    "    city = location.find('city') \n",
    "    times = location.find_all('tmef')\n",
    "    values = location.find_all('wf')\n",
    "    \n",
    "    city_name = city.get_text()\n",
    "    for i in range(len(times)):\n",
    "        weather = values[i].get_text()\n",
    "        day = times[i].get_text()\n",
    "        \n",
    "        time_weather[day] = weather\n",
    "\n",
    "    df[city_name] = time_weather\n",
    "#    indexing_time = time\n",
    "#    df.set_index(time_weather.keys())\n",
    "\n",
    "df.to_csv('/Users/Mac/Desktop/weather.csv')\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<font color=\"blue\" size=\"3em\"><b>네이버 영화 댓글 크롤링 프로젝트</b></font>\n",
    "http://movie.naver.com/\n",
    "\n",
    "<font size=\"3em\"><b>요구사항:</b></font><br>\n",
    "http://movie.naver.com/movie/running/current.nhn 페이지(현재 상영영화)에서 <br>\n",
    "<br>\n",
    "(1) 예매순 1위 ~ 5위에 해당하는 영화 각각의<br>\n",
    "(2) 개봉후 평점글(140자 평)을 <br>\n",
    "(3) 호감순 100개 읽어서 출력하기<br>\n",
    "<br>\n",
    "<font size=\"3em\"><b>출력 포멧:</b></font><br>\n",
    "[영화 이름] (호감순 140자 평)<br>\n",
    "1]호감순 140자 평1<br>\n",
    "2]호감순 140자 평1<br>\n",
    "<b>.</b><br>\n",
    "<b>.</b><br>\n",
    "<b>.</b><br>\n",
    "100]호감순 140자 평1<br>\n",
    "<br>\n",
    "<font size=\"3em\"><b>고려 사항:</b></font><br>\n",
    "140자 평이 100개 이하일 경우에는 140자 평 갯수만큼 출력할 것<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://movie.naver.com/movie/running/current.nhn')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "movie_list = {}\n",
    "movies = soup.select(\"#content > div.article > div > div.lst_wrap > ul > li\")\n",
    "\n",
    "for n in range(5):\n",
    "    movie = movies[n].find_all(\"a\")\n",
    "    movie_list[movie[1].string] = \"http://movie.naver.com\"+movie[0].get('href').replace(\"basic\", \"pointWriteFormList\")+\"&type=after&page=\"\n",
    "        \n",
    "for movie_name, movie_href in movie_list.items():\n",
    "    print('[{}] (호감순 140자 평)'.format(movie_name))\n",
    "    comments = []\n",
    "    i=0\n",
    "    loop = True\n",
    "    while loop and i < 10:\n",
    "        i+=1\n",
    "        try:\n",
    "            res = requests.get(movie_href+str(i))\n",
    "            soup = BeautifulSoup(res.content, 'html.parser')\n",
    "            comment_list = soup.select(\"body > div > div > div.score_result > ul\")\n",
    "            for comment in comment_list[0].find_all(\"p\"):\n",
    "                comments.append(comment.text.replace('BEST','').replace('관람객',''))\n",
    "        except:\n",
    "            loop = False\n",
    "        \n",
    "    for i, comment in enumerate(comments):\n",
    "        print('{}] {}'.format(i+1, comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네이버 경제 기사 크롤링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests \n",
    "import re \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Get the news for an year\n",
    "for i in range(365):\n",
    "    today = datetime.date.today()\n",
    "    today = today + datetime.timedelta(-i)\n",
    "    today = (\"\").join(str(today).split('-'))\n",
    "    print(today)\n",
    "    naver_news_today= \"http://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&sectionId=101&date={}\".format(today)\n",
    "    print(naver_news_today)\n",
    "    res = requests.get(naver_news_today)\n",
    "    soup = bs(res.content, 'html.parser')\n",
    "    try:\n",
    "    # get rinks\n",
    "        rinks = []\n",
    "        # top3\n",
    "        top3 = soup.find('div', class_=\"ranking_top3\")\n",
    "        top3 = top3.select(\"a[href]\")\n",
    "        for top in top3:\n",
    "            rinks.append(re.search('/main/\\w+/\\w+[.a-zA-Z?=&0-9;_]+', top['href']).group())\n",
    "        rinks = rinks[::2]\n",
    "        # 4 to 30th ranked news\n",
    "        for rank in range(4,31):\n",
    "            all_= soup.find('li', class_=\"gnum{}\".format(rank))\n",
    "            all_ = all_.select(\"a[href]\")\n",
    "            for al in all_:\n",
    "                rinks.append(re.search('/main/\\w+/\\w+[.a-zA-Z?=&0-9_]+', al['href']).group())\n",
    "        \n",
    "        # gathering the day's news\n",
    "        articles = '{}'.format(today)\n",
    "        for rink in rinks:\n",
    "            res = requests.get(base_url+rink)\n",
    "            soup = bs(res.content, 'html.parser')\n",
    "            article = re.sub(r'[\\n]', '', soup.find(\"div\", id=\"articleBodyContents\").text)\n",
    "            article = re.sub(r'[\\w0-9a-zA-Z_.-]@[\\w0-9a-zA-Z_.-]', '', article)\n",
    "            article = re.sub(r'\\w+\\.\\w+.\\w+', '', article)\n",
    "            article = re.sub(r'무단전재 및 재배포 금지', '', article)\n",
    "            article = re.sub(r'▶.*', '', article )\n",
    "            articles += article \n",
    "        \n",
    "        # make txt files\n",
    "        with open(\"article{}.txt\".format(i), \"w\") as f:\n",
    "            f.write(articles)\n",
    "        \n",
    "    except : \n",
    "        print(\"{} is missed\".format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
