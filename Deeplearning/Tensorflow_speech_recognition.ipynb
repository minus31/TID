{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import librosa\n",
    "import cv2\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def load_audio_file(file_path):\n",
    "    input_length = 16000\n",
    "    data = librosa.core.load(file_path, sr=16000)[0]\n",
    "    if len(data) >= input_length:\n",
    "        data = data[:input_length]\n",
    "    else:\n",
    "        data = np.pad(data, (0, 16000 - len(data)), \"constant\", constant_values=0)\n",
    "    return data\n",
    "\n",
    "def get_spectdata(wav, sr=16000, size=12800):\n",
    "    spect = librosa.feature.melspectrogram(wav, sr=sr, hop_length=161, n_fft=2048)\n",
    "    log_spect = librosa.core.amplitude_to_db(spect)\n",
    "    data = np.asarray(log_spect).reshape(size)\n",
    "    return data, log_spect\n",
    "\n",
    "def speed_tuning(wav):\n",
    "    speed_rate = np.random.uniform(0.9, 1.1)\n",
    "    wav_speed_tune = cv2.resize(wav, (1, int(len(wav) * speed_rate))).squeeze()\n",
    "\n",
    "    if len(wav_speed_tune) < 16000:\n",
    "        pad_len = 16000 - len(wav_speed_tune)\n",
    "        wav_speed_tune = np.r_[np.random.uniform(-0.001, 0.001, int(pad_len / 2)),\n",
    "                               wav_speed_tune,\n",
    "                               np.random.uniform(-0.001, 0.001, int(np.ceil(pad_len / 2)))]\n",
    "    else:\n",
    "        cut_len = len(wav_speed_tune) - 16000\n",
    "        wav_speed_tune = wav_speed_tune[int(cut_len / 2): int(cut_len / 2) + 16000]\n",
    "    return wav_speed_tune\n",
    "\n",
    "def pitch_tuning(wav, sample_rate=16000):\n",
    "    bins_per_octave = 24\n",
    "    pitch_pm = 4\n",
    "    pitch_change = pitch_pm * 2 * (np.random.uniform() - 0.5)\n",
    "\n",
    "    wav_pitch_changed = librosa.effects.pitch_shift(wav.astype('float64'),\n",
    "                                                    sample_rate,\n",
    "                                                    n_steps=pitch_change,\n",
    "                                                    bins_per_octave=bins_per_octave)\n",
    "    return wav_pitch_changed\n",
    "\n",
    "def bg_mixing(wav, bg):\n",
    "    start_ = np.random.randint(bg.shape[0] - 16000)\n",
    "    bg_slice = bg[start_: start_ + 16000]\n",
    "    wav_with_bg = wav * np.random.uniform(0.8, 1.2) + bg_slice * np.random.uniform(0, 0.1)\n",
    "    return wav_with_bg\n",
    "\n",
    "def choice_bg():\n",
    "    return librosa.load(bg_path + bg_list[np.random.randint(0, 5)], sr=None)[0]\n",
    "\n",
    "\n",
    "target_label = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"unknown\", \"silence\"]\n",
    "valid_labels = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"silence\"]\n",
    "\n",
    "lb = LabelBinarizer().fit(target_label)\n",
    "\n",
    "bg_path = './train/audio/_background_noise_/'\n",
    "bg_list = os.listdir(bg_path)\n",
    "bg_list.remove('README.md')\n",
    "\n",
    "train_audio_path = './train/audio/'\n",
    "test_audio_path = './test/audio/'\n",
    "labels = os.listdir(train_audio_path)\n",
    "\n",
    "train_path = \"./tfrecord_new/train.tfrecord\"\n",
    "test_path = \"./tfrecord_new/test.tfrecord\"\n",
    "\n",
    "train_writer = tf.python_io.TFRecordWriter(train_path)\n",
    "test_writer = tf.python_io.TFRecordWriter(test_path)\n",
    "\n",
    "\n",
    "for label in labels:\n",
    "    if label == '.DS_Store':\n",
    "        continue\n",
    "    else:\n",
    "        orig_label = label\n",
    "        file_path = train_audio_path + label + \"/\"\n",
    "        files = os.listdir(file_path)\n",
    "\n",
    "        if label not in valid_labels:\n",
    "            label = \"unknown\"\n",
    "\n",
    "        encoded_label = lb.transform([label])[0]\n",
    "\n",
    "        for file in files:\n",
    "            if file == '.DS_Store' or file == 'README.md':\n",
    "                continue\n",
    "            else:\n",
    "                filename = file_path + file\n",
    "\n",
    "                signal = load_audio_file(filename)\n",
    "\n",
    "                signal_dict = {}\n",
    "\n",
    "                signal_dict[0] = signal\n",
    "                signal_dict[1] = speed_tuning(signal)\n",
    "                signal_dict[2] = pitch_tuning(signal)\n",
    "\n",
    "                bg = choice_bg()\n",
    "                signal_dict[3] = bg_mixing(signal, bg)\n",
    "\n",
    "                bg = choice_bg()\n",
    "                signal_dict[4] = bg_mixing((speed_tuning(signal)), bg)\n",
    "\n",
    "                signal_dict[5] = speed_tuning(pitch_tuning(signal))\n",
    "\n",
    "                bg = choice_bg()\n",
    "                signal_dict[6] = bg_mixing(pitch_tuning(signal), bg)\n",
    "\n",
    "                bg = choice_bg()\n",
    "                signal_dict[7] = bg_mixing(speed_tuning(pitch_tuning(signal)), bg)\n",
    "\n",
    "                specs = [get_spectdata(signal_dict[j])[0] for j in signal_dict.keys()]\n",
    "\n",
    "                for spec in specs:\n",
    "                    feature = {\n",
    "                        \"spectrum\": float_feature(spec),\n",
    "                        \"label\": int64_feature(encoded_label)\n",
    "                    }\n",
    "\n",
    "                    features = tf.train.Features(feature=feature)\n",
    "                    example = tf.train.Example(features=features)\n",
    "                    train_writer.write(example.SerializeToString())\n",
    "\n",
    "train_writer.close()\n",
    "\n",
    "file_path = test_audio_path\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "for file in files:\n",
    "    if file == '.DS_Store' or file == 'README.md':\n",
    "        continue\n",
    "    else:\n",
    "        filename = file_path + file\n",
    "\n",
    "        signal = load_audio_file(filename)\n",
    "        spec = get_spectdata(signal)[0]\n",
    "\n",
    "        print(spec.shape)\n",
    "\n",
    "        feature = {\n",
    "            \"spectrum\": float_feature(spec),\n",
    "        }\n",
    "\n",
    "        features = tf.train.Features(feature=feature)\n",
    "        example = tf.train.Example(features=features)\n",
    "        test_writer.write(example.SerializeToString())\n",
    "\n",
    "test_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DenseNet:\n",
    "    def batch_norm_relu(self, inputs, is_training, reuse, name):\n",
    "        bn = tf.layers.batch_normalization(inputs, \n",
    "                                           training=is_training, \n",
    "                                           reuse=reuse, \n",
    "                                           name=name)\n",
    "        outputs = tf.nn.relu(bn)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def initial_conv(self, inputs, reuse=False):\n",
    "        l = tf.layers.conv2d(inputs=inputs,\n",
    "                             filters=16,\n",
    "                             kernel_size=3,\n",
    "                             strides=2,\n",
    "                             padding='SAME',\n",
    "                             name='init_conv',\n",
    "                             reuse=reuse)\n",
    "        \n",
    "        return l\n",
    "\n",
    "    \n",
    "    def composite_layer(self, inputs, keep_prob, name, is_training=True, reuse=False):\n",
    "        l = inputs\n",
    "        l = self.batch_norm_relu(l, is_training, reuse, name=name+'_bn1')\n",
    "        l = tf.layers.conv2d(l, 4 * 12, 1, 1, \n",
    "                             padding='SAME', name=name+'_conv1', reuse=reuse)\n",
    "            \n",
    "        l = self.batch_norm_relu(l, is_training, reuse, name=name+'_bn2')\n",
    "        \n",
    "        l = tf.layers.conv2d(l, 12, 3, 1, \n",
    "                             padding='SAME', name=name+'_conv2', reuse=reuse)\n",
    "        \n",
    "        l = tf.layers.dropout(l, keep_prob, training=is_training)\n",
    "        \n",
    "        return tf.concat([inputs, l], axis=3) \n",
    "\n",
    "\n",
    "    def transition_layer(self, inputs, name, is_training=True, reuse=False):\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        n_filters = int(shape[3] * 0.5)\n",
    "        \n",
    "        l = self.batch_norm_relu(inputs, is_training, reuse, name=name + '_bn')\n",
    "        l = tf.layers.conv2d(l, n_filters, 1, 1, padding='SAME', name=name + '_conv', reuse=reuse)\n",
    "        l = tf.layers.average_pooling2d(l, 2, 2, name='pool')\n",
    "\n",
    "        return l\n",
    "    \n",
    "    def dense_net(self, inputs, keep_prob=0.2, is_training=True, reuse=False):\n",
    "        l = self.initial_conv(inputs=inputs, reuse=reuse)\n",
    "        \n",
    "        with tf.variable_scope('block1') as scope:\n",
    "            for i in range(6):\n",
    "                l = self.composite_layer(l, \n",
    "                                         keep_prob, \n",
    "                                         name='dense_layer{}'.format(i), \n",
    "                                         is_training=is_training,\n",
    "                                         reuse=reuse)\n",
    "                \n",
    "            l = self.transition_layer(l, \n",
    "                                     name='transition1',\n",
    "                                     is_training=is_training,\n",
    "                                     reuse=reuse)\n",
    "                \n",
    "        with tf.variable_scope('block2') as scope:\n",
    "            for i in range(12):\n",
    "                l = self.composite_layer(l, \n",
    "                                         keep_prob, \n",
    "                                         name='dense_layer{}'.format(i),\n",
    "                                         is_training=is_training,\n",
    "                                         reuse=reuse)\n",
    "\n",
    "            l = self.transition_layer(l, \n",
    "                                     name='transition2',\n",
    "                                     is_training=is_training,\n",
    "                                     reuse=reuse)\n",
    "\n",
    "        with tf.variable_scope('block3') as scope:\n",
    "            for i in range(24):\n",
    "                l = self.composite_layer(l, \n",
    "                                         keep_prob, \n",
    "                                         name='dense_layer{}'.format(i),\n",
    "                                         is_training=is_training,\n",
    "                                         reuse=reuse)\n",
    "                \n",
    "            l = self.transition_layer(l, \n",
    "                                     name='transition3',\n",
    "                                     is_training=is_training,\n",
    "                                     reuse=reuse)\n",
    "\n",
    "        with tf.variable_scope('block4') as scope:\n",
    "            for i in range(16):\n",
    "                l = self.composite_layer(l, \n",
    "                                         keep_prob, \n",
    "                                         name='dense_layer{}'.format(i),\n",
    "                                         is_training=is_training,\n",
    "                                         reuse=reuse)\n",
    "                \n",
    "        return l\n",
    "    \n",
    "    \n",
    "    def get_logits(self, inputs, is_training=True, reuse=False):\n",
    "        l = self.dense_net(inputs, keep_prob=0.2, is_training=is_training, reuse=reuse)\n",
    "        \n",
    "        outputs = self.batch_norm_relu(l, is_training, reuse, name='last_bn')\n",
    "\n",
    "        shape = outputs.get_shape().as_list()\n",
    "        \n",
    "        pool_size = (shape[1], shape[2])\n",
    "        outputs= tf.layers.average_pooling2d(outputs, pool_size=pool_size, strides=1, padding='VALID')\n",
    "        \n",
    "        outputs = tf.layers.flatten(outputs)\n",
    "        outputs = tf.layers.dense(outputs, 12, name='final_dense', reuse=reuse)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class CnnLstm:\n",
    "    def __init__(self):\n",
    "        self.num_classes = 12\n",
    "        self.num_filters = [8, 16, 32, 32]\n",
    "        self.filter_sizes = [7, 3, 3, 3]\n",
    "        self.pool_sizes = [2, 2, 1, 1]\n",
    "        self.cnn_dropout_keep_prob = [0, 0.3, 0.4, 0.4]\n",
    "        self.fc_hidden_units = [1028, 512, 256]\n",
    "        self.fc_dropout_keep_prob = [0.2, 0.3, 0.35]\n",
    "        self.lstm_n_hiddens = [512]\n",
    "        self.lstm_dropout_keep_prob = [0.5]\n",
    "        self.idx_convolutional_layers = range(1, len(self.filter_sizes) + 1)\n",
    "        self.idx_fc_layers = range(1, len(self.fc_hidden_units) + 1)\n",
    "        self.idx_lstm_layers = range(1, len(self.lstm_n_hiddens) + 1)\n",
    "        \n",
    "        \n",
    "    def convolutional_layer(self, inputs, is_training=True, reuse=False):\n",
    "        l = inputs\n",
    "        \n",
    "        for i, num_filter, filter_size, pool_size, keep_prob in zip(self.idx_convolutional_layers,\n",
    "                                                                    self.num_filters,\n",
    "                                                                    self.filter_sizes,\n",
    "                                                                    self.pool_sizes,\n",
    "                                                                    self.cnn_dropout_keep_prob):\n",
    "            l = tf.layers.conv2d(l, \n",
    "                                 filters=num_filter, \n",
    "                                 kernel_size=filter_size, \n",
    "                                 strides=1, \n",
    "                                 padding=\"SAME\", \n",
    "                                 name=\"conv\"+str(i),\n",
    "                                 reuse=reuse)\n",
    "            \n",
    "            l = tf.layers.batch_normalization(l, training=is_training, name=\"conv_bn\"+str(i), reuse=reuse)\n",
    "            l = tf.nn.relu(l, name=\"conv_relu\"+str(i))\n",
    "            l = tf.layers.dropout(l, rate=keep_prob, training=is_training, name=\"conv_dropout\"+str(i))\n",
    "\n",
    "            if pool_size != 1:\n",
    "                l = tf.layers.max_pooling2d(l, pool_size=pool_size, strides=pool_size, padding=\"SAME\")\n",
    "                \n",
    "        return l\n",
    "        \n",
    "    \n",
    "    def fc_layer(self, inputs, is_training=True, reuse=False):\n",
    "        l = inputs\n",
    "        \n",
    "        for i, units, keep_prob in zip(self.idx_fc_layers, self.fc_hidden_units, self.fc_dropout_keep_prob):\n",
    "            l = tf.layers.dense(inputs, units=units, reuse=reuse, name=\"fc\"+str(i))\n",
    "            l = tf.layers.batch_normalization(l, training=is_training, name=\"fc_bn\"+str(i), reuse=reuse)\n",
    "            l = tf.nn.relu(l, name=\"fc_relu\"+str(i))\n",
    "            l = tf.layers.dropout(l, rate=keep_prob, training=is_training, name=\"fc_dropout\"+str(i))\n",
    "            \n",
    "        return l\n",
    "  \n",
    "\n",
    "    def lstm_layer(self, inputs, is_training=True, reuse=False):\n",
    "        if is_training:\n",
    "            keep_probs = [0.5]\n",
    "            \n",
    "        else:\n",
    "            keep_probs = [1]\n",
    "            \n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(512, reuse=reuse)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_probs[0])\n",
    "        \n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = outputs[-1]\n",
    "        \n",
    "        return outputs\n",
    " \n",
    "\n",
    "    def get_reshaped_cnn_to_rnn(self, inputs):\n",
    "        shape = inputs.get_shape().as_list() \n",
    "        inputs = tf.transpose(inputs, [0, 2, 1, 3])\n",
    "        reshaped_inputs = tf.reshape(inputs, [-1, shape[2], shape[1] * shape[3]])\n",
    "        \n",
    "        return reshaped_inputs\n",
    "  \n",
    "\n",
    "    def get_logits(self, inputs, is_training=True, reuse=False):\n",
    "        with tf.variable_scope(\"conv_layers\") as scope:\n",
    "            l = inputs\n",
    "            l = self.convolutional_layer(l, is_training, reuse)\n",
    "            \n",
    "        with tf.variable_scope(\"lstm_layers\") as scope:\n",
    "            reshaped_l = self.get_reshaped_cnn_to_rnn(l)\n",
    "            \n",
    "            l = self.lstm_layer(reshaped_l, is_training, reuse)\n",
    "            \n",
    "        with tf.variable_scope(\"fc_layers\") as scope:\n",
    "            l = tf.layers.flatten(l)\n",
    "            l = self.fc_layer(l, is_training, reuse)\n",
    "                \n",
    "        output = tf.layers.dense(l, units=self.num_classes, reuse=reuse, name='out')\n",
    "            \n",
    "        return output\n",
    "    \n",
    "\n",
    "def train_parser(serialized_example):\n",
    "    features = {\n",
    "        \"spectrum\": tf.FixedLenFeature([12800], tf.float32),\n",
    "        \"label\": tf.FixedLenFeature([12], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed_feature = tf.parse_single_example(serialized_example, features)\n",
    "\n",
    "    spec = parsed_feature['spectrum']\n",
    "    label = parsed_feature['label']\n",
    "\n",
    "    return spec, label\n",
    "        \n",
    "    \n",
    "def test_parser(serialized_example):\n",
    "    features = {\n",
    "        \"spectrum\": tf.FixedLenFeature([12800], tf.float32),\n",
    "    }\n",
    "\n",
    "    parsed_feature = tf.parse_single_example(serialized_example, features)\n",
    "\n",
    "    spec = parsed_feature['spectrum']\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from model import DenseNet, CnnLstm, train_parser\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_data_dir = \"./tfrecords/train.tfrecord\"\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(train_data_dir).map(train_parser)\n",
    "train_dataset = train_dataset.shuffle(500000, seed=1, reshuffle_each_iteration=True)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "train_itr = tf.contrib.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "\n",
    "spec, label = train_itr.get_next()\n",
    "spec = tf.reshape(spec, [-1, 128, 100, 1])\n",
    "spec = tf.cast(spec, tf.float32)\n",
    "\n",
    "train_init_op = train_itr.make_initializer(train_dataset)\n",
    "\n",
    "model = DenseNet()\n",
    "# model = CnnLstm()\n",
    "\n",
    "height = 128\n",
    "width = 100\n",
    "num_classes = 12\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "\n",
    "model_path = \"./densenet/\"\n",
    "model_file = \"densenet\"\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    X = tf.placeholder(tf.float32, [None, height, width, 1])\n",
    "    Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "    logits_train = model.get_logits(X)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(Y, logits_train)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        \n",
    "    logits_eval = model.get_logits(X, is_training=False, reuse=True)\n",
    "    predict_proba_ = tf.nn.softmax(logits_eval)\n",
    "    prediction = tf.argmax(predict_proba_, 1)\n",
    "    accuracy = tf.metrics.accuracy(tf.argmax(Y, 1), prediction)\n",
    "                \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "        \n",
    "for epoch in range(epochs):\n",
    "    sess.run(train_init_op)\n",
    "    acc = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            step = sess.run(global_step)\n",
    "            \n",
    "            _spec, _label = sess.run([spec, label])\n",
    "                \n",
    "            _, c, _summ = sess.run([optimizer, loss, merged], feed_dict = {X: _spec, Y: _label})\n",
    "            acc_train = sess.run(accuracy, feed_dict = {X: _spec, Y: _label})\n",
    "            \n",
    "            acc.append(acc_train[1])\n",
    "            \n",
    "            writer.add_summary(_summ, step)\n",
    "            \n",
    "            if step % 500 == 0:\n",
    "                print('step: {}, cost: {}'.format(step, c))\n",
    "                \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "            \n",
    "    print('epoch: {}, cost : {}, train_acc: {}'.format(epoch, c, np.mean(acc)))\n",
    "\n",
    "\n",
    "saver.save(sess, model_path + model_file + '.ckpt', global_step=sess.run(global_step))\n",
    "\n",
    "print(\"Model is saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from model import DenseNet, CnnLstm, test_parser\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "test_data_dir = \"./tfrecords/test.tfrecord\"\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(test_data_dir).map(test_parser)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "test_itr = tf.contrib.data.Iterator.from_structure(test_dataset.output_types, test_dataset.output_shapes)\n",
    "\n",
    "test_spec = test_itr.get_next()\n",
    "\n",
    "test_spec = tf.reshape(test_spec, [-1, 128, 100, 1])\n",
    "test_spec = tf.cast(test_spec, tf.float32)\n",
    "\n",
    "test_init_op = test_itr.make_initializer(test_dataset)\n",
    "\n",
    "height = 128\n",
    "width = 100\n",
    "\n",
    "model = DenseNet()\n",
    "# model = CnnLstm()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    X = tf.placeholder(tf.float32, [None, height, width, 1])\n",
    "    \n",
    "    logits_test = model.get_logits(X, is_training=False, reuse=False)\n",
    "    test_predict_proba_ = tf.nn.softmax(logits_test)\n",
    "    test_prediction = tf.argmax(test_predict_proba_, 1)\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "\n",
    "# Restore model\n",
    "imp_model = tf.train.import_meta_graph('./densenet/densenet.ckpt-44810.meta')\n",
    "imp_model.restore(sess, tf.train.latest_checkpoint('./densenet/'))\n",
    "\n",
    "\n",
    "# Create submission file\n",
    "sess.run(test_init_op)\n",
    "\n",
    "test_spec_ = sess.run(test_spec)\n",
    "\n",
    "predict = sess.run(test_prediction, feed_dict={X: test_spec_})\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        test_spec_ = sess.run(test_spec)\n",
    "\n",
    "        predict = np.hstack([predict, sess.run(test_prediction, feed_dict={X: test_spec_})])\n",
    "        \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "\n",
    "class_names = ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'silence', 'stop', 'unknown', 'up', 'yes']\n",
    "\n",
    "df = pd.read_csv(\"sample_submission.csv\")\n",
    "files = df['fname']\n",
    "\n",
    "model_path = \"./densenet/\"\n",
    "model_file = \"densenet\"\n",
    "\n",
    "with open(model_path + 'sub_' + model_file + '.csv', 'w') as f:\n",
    "    fieldnames = ['fname', 'label']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    for i in range(len(predict)):\n",
    "        writer.writerow({'fname': files[i], 'label': class_names[predict[i]]})\n",
    "        \n",
    "print(\"Submission file is created.\")\n",
    "\n",
    "\n",
    "# Create predict proba file\n",
    "sess.run(test_init_op)\n",
    "\n",
    "test_spec_ = sess.run(test_spec)\n",
    "\n",
    "predict_proba = sess.run(test_predict_proba_, feed_dict={X: test_spec_})\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        test_spec_ = sess.run(test_spec)\n",
    "        \n",
    "        predict_proba = np.vstack([predict_proba, sess.run(test_predict_proba_, feed_dict={X: test_spec_})])\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "        \n",
    "predict_proba = np.array(predict_proba)\n",
    "print(predict_proba.shape)\n",
    "\n",
    "pp = pd.DataFrame(predict_proba, index=files)\n",
    "pp.to_csv(model_path + 'proba_' + model_file + '.csv', index=False)\n",
    "\n",
    "print(\"Proba file is created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
